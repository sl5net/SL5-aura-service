# warm_up_cache_v03.py
# Version: 0.3
# Status: Aura-Safe (Crasht nicht beim Laden durch den Service)

import sys
import os
import re
import subprocess
import time
from pathlib import Path

# --- AURA SAFETY CHECK ---
# Wir definieren ask_ollama global als None.
# Wenn der Import fehlschl√§gt (weil Aura es l√§dt), crashen wir NICHT sofort.
ask_ollama = None

try:
    # Versuchen, das Modul zu laden (f√ºr manuelle Ausf√ºhrung)
    # Wir f√ºgen den aktuellen Ordner zum Pfad hinzu, damit Importe klappen
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    import ask_ollama
except ImportError:
    # Wenn das fehlschl√§gt, machen wir gar nichts.
    # Aura wird diese Datei laden, sehen dass nichts passiert, und weitermachen.
    pass


# --- HELPER: README FINDEN ---
def get_readme_content_standalone():
    """Sucht die README.md eigenst√§ndig."""
    try:
        current_path = Path(__file__).resolve()
        for _ in range(6):
            current_path = current_path.parent
            readme_path = current_path / "README.md"
            if readme_path.exists():
                print(f"üìÑ README gefunden: {readme_path}")
                content = readme_path.read_text(encoding='utf-8').strip()
                return content[:6000]
        print("‚ùå WARNUNG: Keine README.md gefunden.")
        return None
    except Exception as e:
        print(f"‚ùå Fehler beim Lesen der README: {e}")
        return None


# --- LLM FRAGEN GENERATOR ---
def generate_questions_via_llm(readme_text):
    print("üß† Aura liest die README, um Fragen zu generieren...")

    prompt = (
        "Du bist ein QA-Engineer. Analysiere die folgende Dokumentation.\n"
        "Erstelle eine Liste mit den 5 h√§ufigsten Fragen, die ein Nutzer dazu stellen w√ºrde.\n"
        "Format: Nur die Fragen, eine pro Zeile. Keine Nummerierung. Deutsch.\n"
        "Beispiele: 'Wie installiere ich das?', 'Welche Features gibt es?'\n\n"
        f"DOKUMENTATION:\n{readme_text}\n"
    )

    cmd = ["ollama", "run", "llama3.2"]

    # Nutzung von STDIN (Pipe) f√ºr Stabilit√§t
    result = subprocess.run(cmd, input=prompt, capture_output=True, text=True, encoding='utf-8')

    if result.returncode != 0:
        print(f"‚ùå Fehler bei Ollama: {result.stderr}")
        return []

    raw_lines = result.stdout.strip().split('\n')
    questions = []
    for line in raw_lines:
        clean_line = re.sub(r'^[\d\-\.\s]+', '', line).strip()
        if clean_line and "?" in clean_line:
            questions.append(clean_line)

    return questions


# --- MOCK OBJEKTE ---
class MockMatchObj:
    def __init__(self, text):
        self.text = text

    def groups(self):
        return ("Computer", self.text)

    def group(self, index):
        if index == 2: return self.text
        return "Computer"


def simulate_aura_request(question):
    # Sicherheitscheck: Wenn ask_ollama fehlt, k√∂nnen wir nicht simulieren
    if ask_ollama is None:
        print(f"‚ö†Ô∏è  √úberspringe '{question}' (ask_ollama Modul nicht geladen)")
        return

    print(f"ü§ñ Simuliere Frage: '{question}'")

    match_data = {
        'regex_match_obj': MockMatchObj(question)
    }

    start = time.time()
    try:
        response = ask_ollama.execute(match_data)
        duration = time.time() - start
        preview = response.replace('\n', ' ')[:80] if response else "Keine Antwort"
        print(f"    ‚è±Ô∏è  Dauer: {duration:.2f}s")
        print(f"    üí¨ Antwort: {preview}...")
    except Exception as e:
        print(f"    ‚ùå Fehler bei der Ausf√ºhrung: {e}")
    print("-" * 40)


def main():
    # --- WICHTIG: Erst hier pr√ºfen wir den Import hart ---
    if ask_ollama is None:
        print("\n‚ùå FATAL ERROR: Konnte 'ask_ollama.py' nicht importieren.")
        print("   Bitte stelle sicher, dass 'ask_ollama.py' im selben Ordner liegt.")
        print("   Dieser Fehler ist normal, wenn Aura das Skript automatisch l√§dt,")
        print("   aber nicht, wenn du es manuell startest.\n")
        sys.exit(1)

    print("üî• Starting Cache Warmer v0.3 for SL5 Aura...")
    print("=============================================")

    try:
        os.system("rm -rf __pycache__")
    except Exception:
        pass

    readme = get_readme_content_standalone()
    if not readme:
        print("‚ùå Abbruch: Keine Doku gefunden.")
        return

    questions = generate_questions_via_llm(readme)
    if not questions:
        print("‚ùå Abbruch: LLM hat keine Fragen generiert.")
        return

    print(f"‚úÖ Habe {len(questions)} Fragen generiert.")
    print("=" * 60)

    for q in questions:
        simulate_aura_request(q)

    print("=" * 60)
    print("üéâ Cache Warming abgeschlossen! Datenbank ist gef√ºllt.")


if __name__ == "__main__":
    main()
